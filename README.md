# Analysis of Stock Data using Pyspark

-> During the process of analyzing blockchain data using Apache Spark and PySpark, several challenges and solutions emerged. Initially, while loading the data from an HDFS location into Spark DataFrames, a Unicode error occurred during schema inference. This was resolved by ensuring consistent data types across columns and explicitly defining the schema when creating the DataFrame. Additionally, Python 2 compatibility issues were addressed, with code examples provided for tasks like counting the total number of blocks, identifying the block with the largest height, retrieving the date and time of that block, and finding the block with the highest number of transactions. These tasks utilized Spark RDDs and Spark SQL to derive key insights, emphasizing the importance of clean, consistent data and explicit schema definition for accurate analysis within the Spark environment.

-> In a related project, Apache Spark and its SQL module were used to analyze data stored in MySQL databases. The data was spread across three tables: blocks, blocks_info, and tx_info. Using PySparkâ€™s SQL capabilities, various queries were executed to extract insights. First, it was determined that the blocks table contained 807,290 total blocks. The block with the highest height was identified as 807,290, and its timestamp was found in the blocks_info table. Further analysis revealed that this block also had the highest number of transactions. This project demonstrated the efficiency of Spark SQL in querying and analyzing data from MySQL databases, providing important insights into the structure of the blockchain dataset.

-> In another analysis using Spark SQL, stock market data stored on HDFS was examined. This dataset included columns like Symbol, Timestamp, Open, Close, High, Low, and Volume. Using Spark SQL and DataFrames, various queries were performed, including counting the total records, identifying distinct days, and calculating the number of records per day. Additionally, the analysis identified unique symbols, computed the highest, lowest, and average prices for each symbol, determined their price ranges, and found the dates when each symbol reached its highest price. Challenges such as column naming issues and data type inconsistencies were resolved, leading to valuable insights into stock market patterns and trends for each symbol over time.
